

## Tutorials
1. How to setup inference service: https://www.youtube.com/watch?v=5i_LyrjFm7I


## Efficient Serving libraries
1. vllm: https://github.com/vllm-project/vllm
2. local AI assistant: https://github.com/janhq/jan
3. local LLM: https://github.com/ollama/ollama
4. onnx: https://github.com/onnx/onnx
5. Nvidia NIM